---
title: 'Programming in Python: Lecture 8'
author: "Carolyn Jane Anderson"
date: "11/3/2017"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

Today we're going to take a look at the NLTK package. First up: corpora.

<h2>Corpora</h2>

NLTK includes many useful collections of texts:

Australian Broadcasting Commission 2006 

Brown Corpus 

Chat-80 Data Files 

City Database 

The Carnegie Mellon Pronouncing Dictionary (0.6) 

Comparative Sentence Dataset 

Crubadan Corpus 

Dolch Word List 

Sample European Parliament Proceedings Parallel Corpus 

Gazeteer Lists 

Genesis Corpus 

Project Gutenberg Selections 

C-Span Inaugural Address Corpus 

KNB Corpus (Annotated blog corpus) 

PanLex Swadesh Corpora 

The Patient Information Leaflet (PIL) Corpus 

Polish language of the XX century sixties 

Problem Report Corpus 

Product Reviews 

Shakespeare XML Corpus Sample 

C-Span State of the Union Address Corpus 

Switchboard Corpus Sample

Twitter Samples 

Universal Declaration of Human Rights Corpus 

Last class, I used the text of Emma from the Gutenberg corpus to train our spell-checker.

<h3>Getting started with corpora</h3>

You can download NLTK resources using the NLTK importer. 

Open a Python terminal and import NLTK. Then type:

```
nltk.download()
```

You should see a new window that lets you select modules from NLTK to download. This is helpful when working with large corpora, because you may not have space on your computer to download all of the corpora NLTK provides.

Now that we have some texts downloaded, we can import them into a Python file and start working with them.

<h5>See corpora.py</h5>

<h2>Part-of-speech tagging</h2>

NLTK provides a number of corpora that have already been part-of-speech tagged. 

<h4>Tagged corpora</h4>

Indian Language POS-Tagged Corpus 

JEITA Public Morphologically Tagged Corpus (in ChaSen format) 

MAC-MORPHO: Brazilian Portuguese news text with part-of-speech tags 

Machado de Assis -- Obra Completa 

MASC Tagged Corpus 

SENSEVAL 2 Corpus: Sense Tagged Text  

It also comes with a few part-of-speech taggers that you can apply to your own data. 

<h4>POS-taggers</h4>

Averaged Perceptron Tagger 

Averaged Perceptron Tagger (Russian) 

Treebank Part of Speech Tagger (HMM) 

Treebank Part of Speech Tagger (Maximum entropy) 

<h5>See pos.py</h5>

<h2>Parsing</h2>

NLTK includes a number of treebanks (collections of parse trees) for various languages.

<h4>Treebanks</h4>

Alpino Dutch Treebank 

CESS-CAT Treebank 

CESS-ESP Treebank 

Dependency Treebanks from CoNLL 2007 (Catalan and Basque Subset)

Dependency Parsed Treebank 

Portuguese Treebank 

Penn Treebank 

Sinica Treebank Corpus Sample 

Universal Treebanks Version 2.0 

York-Toronto-Helsinki Parsed Corpus of Old English Prose

There are also parsing algorithms that NLTK provides, including a Shift Reduce parser, a Recursive Descent parser, the Stanford parser, and an Earley parser.

I'll demo the Earley parser, since we've discussed that in class, but probably the others are better if you want to apply them to a lot of text. 

<h5>See pos.py</h5>

<h2>Word frequencies</h2>

Another thing you might want to do with a text is understand how often different words are used. We can start by visualizing the relative frequencies of words in the text.

<h5>See freq.py</h5>
